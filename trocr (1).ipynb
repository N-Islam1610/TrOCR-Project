{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:17:10.118942Z","iopub.execute_input":"2025-09-24T15:17:10.119188Z","iopub.status.idle":"2025-09-24T15:17:11.395956Z","shell.execute_reply.started":"2025-09-24T15:17:10.119163Z","shell.execute_reply":"2025-09-24T15:17:11.395292Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:17:11.396791Z","iopub.execute_input":"2025-09-24T15:17:11.397123Z","iopub.status.idle":"2025-09-24T15:17:11.400537Z","shell.execute_reply.started":"2025-09-24T15:17:11.397104Z","shell.execute_reply":"2025-09-24T15:17:11.399896Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport zipfile\nimport requests\n\ndef download_and_unzip(url, save_path):\n    print(\"Downloading and extracting dataset...\", end=\"\")\n    response = requests.get(url)\n    with open(save_path, \"wb\") as f:\n        f.write(response.content)\n\n    try:\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(os.path.dirname(save_path))\n        print(\"✅ Done!\")\n    except zipfile.BadZipFile:\n        print(\"❌ Failed to unzip file. The file may be corrupted.\")\n\n# Dataset download URL and paths\nURL = \"https://www.dropbox.com/scl/fi/vyvr7jbdvu8o174mbqgde/scut_data.zip?rlkey=fs8axkpxunwu6if9a2su71kxs&dl=1\"\ndataset_dir = \"work\"\ndataset_zip_path = os.path.join(dataset_dir, \"scut_data.zip\")\n\n# Make sure work/ directory exists\nos.makedirs(dataset_dir, exist_ok=True)\n\n# Download and unzip\ndownload_and_unzip(URL, dataset_zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:17:11.402467Z","iopub.execute_input":"2025-09-24T15:17:11.402930Z","iopub.status.idle":"2025-09-24T15:17:17.844986Z","shell.execute_reply.started":"2025-09-24T15:17:11.402906Z","shell.execute_reply":"2025-09-24T15:17:17.844225Z"}},"outputs":[{"name":"stdout","text":"Downloading and extracting dataset...✅ Done!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import VisionEncoderDecoderModel, TrOCRProcessor\n\nmodel_name = \"microsoft/trocr-base-handwritten\"\nprocessor = TrOCRProcessor.from_pretrained(model_name)\nmodel = VisionEncoderDecoderModel.from_pretrained(model_name)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:17:17.845664Z","iopub.execute_input":"2025-09-24T15:17:17.845859Z","iopub.status.idle":"2025-09-24T15:18:10.733634Z","shell.execute_reply.started":"2025-09-24T15:17:17.845842Z","shell.execute_reply":"2025-09-24T15:18:10.732928Z"}},"outputs":[{"name":"stderr","text":"2025-09-24 15:17:39.656350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758727060.028260      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758727060.132836      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0703a7d6dac4e7288a12a18075f788f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"103cd33551ae498684fecb68c550ca3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c84f2d580b43ba8e8c3ab4bb11f8de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63d6ea5be5c4ef5bd33fc2e97c42cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be150f615434172ae1d4bbc82b65f98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98eb6d76a4704c5599ef6739767ef4dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb9e7b9acb043fa873203f3ea68c12a"}},"metadata":{}},{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e67dc0e03144a8aa50e3753194b4cc"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\ndataset_path = \"/kaggle/working/work/scut_data\"\n\ndef load_data(img_folder, label_file):\n    imgs, texts = [], []\n\n    label_file_path = os.path.join(dataset_path, label_file)\n    img_dir = os.path.join(dataset_path, img_folder)\n\n    with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            parts = line.strip().split(\"\\t\")\n            if len(parts) >= 2:\n                img_name, text = parts[0], \"\\t\".join(parts[1:])\n                imgs.append(os.path.join(img_dir, img_name))\n                texts.append(text)\n\n    return imgs, texts\n\n# Load training and validation sets\ntrain_imgs, train_texts = load_data(\"scut_train\", \"scut_train.txt\")\nval_imgs, val_texts = load_data(\"scut_test\", \"scut_test.txt\")\n\nprint(f\"Train Samples: {len(train_imgs)}, Val Samples: {len(val_imgs)}\")\n\n# Optional: show first 5 samples\nfor i in range(5):\n    print(train_imgs[i], \"->\", train_texts[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:10.734328Z","iopub.execute_input":"2025-09-24T15:18:10.734550Z","iopub.status.idle":"2025-09-24T15:18:10.754164Z","shell.execute_reply.started":"2025-09-24T15:18:10.734525Z","shell.execute_reply":"2025-09-24T15:18:10.753460Z"}},"outputs":[{"name":"stdout","text":"Train Samples: 6052, Val Samples: 1651\n/kaggle/working/work/scut_data/scut_train/000000.jpg -> BARNSTABLE\n/kaggle/working/work/scut_data/scut_train/000001.jpg -> SHERIFF'S OFFICE\n/kaggle/working/work/scut_data/scut_train/000002.jpg -> COUNTY\n/kaggle/working/work/scut_data/scut_train/000003.jpg -> Levelezo-Lap.\n/kaggle/working/work/scut_data/scut_train/000004.jpg -> Brefkort\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================================\n# 2. Model: Load TrOCR Processor and Model\n# ================================================\n\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nimport torch\n\n# Use the smaller pre-trained TrOCR model\nmodel_name = \"microsoft/trocr-small-printed\"\n\n# Load processor (image + tokenizer) and model\nprocessor = TrOCRProcessor.from_pretrained(model_name)\nmodel = VisionEncoderDecoderModel.from_pretrained(model_name)\n\n# Model configuration\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\n\n# Move model to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\nprint(\"✅ TrOCR-small model loaded and ready on device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:10.754867Z","iopub.execute_input":"2025-09-24T15:18:10.755111Z","iopub.status.idle":"2025-09-24T15:18:17.106251Z","shell.execute_reply.started":"2025-09-24T15:18:10.755086Z","shell.execute_reply":"2025-09-24T15:18:17.105551Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2438b47019b944d1a5c7a0bdd64f1f32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/327 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f878d18cd2e049e4b17920d2d1ddcfba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63d550b9fb28485594a35ffacac6e9f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854a1e406f87430fbc8d77b1b9ca95fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9d719090cd47f582047877498519fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9250dc68133240239450ff502729d264"}},"metadata":{}},{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f098179a4b94c469e46cdb32e7fd7af"}},"metadata":{}},{"name":"stdout","text":"✅ TrOCR-small model loaded and ready on device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import editdistance\n\ndef compute_cer(preds, targets):\n    \"\"\"\n    Computes Character Error Rate (CER) between predicted and target texts.\n\n    Args:\n        preds (List[str]): List of predicted strings.\n        targets (List[str]): List of ground truth strings.\n\n    Returns:\n        float: Average CER over all samples.\n    \"\"\"\n    assert len(preds) == len(targets), \"Predictions and targets must be the same length.\"\n\n    total_edits = 0\n    total_chars = 0\n\n    for pred, target in zip(preds, targets):\n        pred = pred.strip().replace(\" \", \"\")\n        target = target.strip().replace(\" \", \"\")\n        total_edits += editdistance.eval(pred, target)\n        total_chars += len(target)\n\n    if total_chars == 0:\n        return 0.0\n\n    return total_edits / total_chars","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:17.107057Z","iopub.execute_input":"2025-09-24T15:18:17.107432Z","iopub.status.idle":"2025-09-24T15:18:17.127206Z","shell.execute_reply.started":"2025-09-24T15:18:17.107412Z","shell.execute_reply":"2025-09-24T15:18:17.126478Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\n# Configuration class for training\nclass TrainingConfig:\n    BATCH_SIZE = 2\n    EPOCHS = 10\n    LEARNING_RATE = 5e-5\n    WEIGHT_DECAY = 0.01\n    WARMUP_STEPS = 500\n\n# HuggingFace training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./trocr_finetuned\",\n    #evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n    num_train_epochs=TrainingConfig.EPOCHS,\n    predict_with_generate=True,\n    fp16=True,\n    logging_strategy=\"epoch\",\n    save_total_limit=2,\n    report_to=\"tensorboard\",\n    lr_scheduler_type=\"constant\",\n    dataloader_num_workers=4,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:17.128033Z","iopub.execute_input":"2025-09-24T15:18:17.128274Z","iopub.status.idle":"2025-09-24T15:18:25.298864Z","shell.execute_reply.started":"2025-09-24T15:18:17.128256Z","shell.execute_reply":"2025-09-24T15:18:25.298263Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CustomOCRDataset(Dataset):\n    \"\"\"\n    Custom Dataset class for OCR using TrOCR.\n\n    Args:\n        root_dir (str): Directory containing image files.\n        label_file (str): Path to the label file (image_name <space/tab> label).\n        processor (TrOCRProcessor): Hugging Face processor (tokenizer + image processor).\n        max_target_length (int): Maximum length of target text sequence.\n        augmentations (callable, optional): Image augmentations (e.g., Albumentations or torchvision).\n    \"\"\"\n\n    def __init__(self, root_dir, label_file, processor, max_target_length=128, augmentations=None):\n        self.root_dir = root_dir\n        self.label_file = label_file\n        self.processor = processor\n        self.max_target_length = max_target_length\n        self.augmentations = augmentations\n\n        self.samples = []\n\n        # Read label mappings\n        with open(label_file, 'r', encoding='utf-8') as file:\n            for line in file:\n                line = line.strip()\n\n                # Skip empty lines\n                if not line:\n                    continue\n\n                # Handle space or tab as separator\n                if '\\t' in line:\n                    file_name, label = line.split('\\t', 1)\n                elif ' ' in line:\n                    file_name, label = line.split(' ', 1)\n                else:\n                    continue  # skip badly formatted lines\n\n                # Normalize filename to 6-digit format if needed\n                if file_name.endswith('.jpg') and len(file_name.split('.')[0]) < 6:\n                    file_name = file_name.zfill(10)\n\n                self.samples.append((file_name, label))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        file_name, label = self.samples[idx]\n        img_path = os.path.join(self.root_dir, file_name)\n\n        # Load image\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image not found at {img_path}\")\n\n        # Apply augmentations if provided\n        if self.augmentations:\n            image = self.augmentations(image)\n\n        # Use processor to prepare inputs and labels\n        encoding = self.processor(\n            image,\n            text=label,\n            padding=\"max_length\",\n            max_length=self.max_target_length,\n            return_tensors=\"pt\"\n        )\n\n        # Remove batch dimension (1, ...) → (...)\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:25.301122Z","iopub.execute_input":"2025-09-24T15:18:25.301733Z","iopub.status.idle":"2025-09-24T15:18:25.310268Z","shell.execute_reply.started":"2025-09-24T15:18:25.301715Z","shell.execute_reply":"2025-09-24T15:18:25.309667Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nimport os\n\n# Load all image-text pairs from label file\nwith open(\"work/scut_data/scut_train.txt\", \"r\", encoding=\"utf-8\") as f:\n    all_lines = [line.strip() for line in f if len(line.strip().split()) >= 2]\n\n# Split into train and validation\ntrain_lines, val_lines = train_test_split(all_lines, test_size=0.1, random_state=42)\n\n# Save to temporary split files\ntrain_label_file = \"work/scut_data/train_split.txt\"\nval_label_file = \"work/scut_data/val_split.txt\"\n\nwith open(train_label_file, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train_lines))\n\nwith open(val_label_file, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(val_lines))\n\n# Define image root directory\nimage_root = \"work/scut_data/scut_train\"\n\n# Now instantiate the datasets\ntrain_dataset = CustomOCRDataset(\n    root_dir=image_root,\n    label_file=train_label_file,\n    processor=processor,\n    max_target_length=64\n)\n\nval_dataset = CustomOCRDataset(\n    root_dir=image_root,\n    label_file=val_label_file,\n    processor=processor,\n    max_target_length=64\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:25.310936Z","iopub.execute_input":"2025-09-24T15:18:25.311191Z","iopub.status.idle":"2025-09-24T15:18:25.363749Z","shell.execute_reply.started":"2025-09-24T15:18:25.311163Z","shell.execute_reply":"2025-09-24T15:18:25.363132Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List\nimport torch\n\n@dataclass\nclass DataCollatorForOCR:\n    processor: Any\n    padding: bool = True\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Get pixel values and already-tokenized label IDs\n        pixel_values = [f[\"pixel_values\"] for f in features]\n        input_ids = [f[\"labels\"] for f in features]  # These are already token IDs!\n\n        # DO NOT call tokenizer() again — just pad\n        batch = self.processor.tokenizer.pad(\n            {\"input_ids\": input_ids},\n            padding=self.padding,\n            return_tensors=\"pt\"\n        )\n\n        # Set padding tokens to -100 so they are ignored in loss\n        labels = batch[\"input_ids\"]\n        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n\n        # Stack pixel values\n        pixel_values = torch.stack(pixel_values)\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": labels\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:25.364391Z","iopub.execute_input":"2025-09-24T15:18:25.364630Z","iopub.status.idle":"2025-09-24T15:18:25.370932Z","shell.execute_reply.started":"2025-09-24T15:18:25.364608Z","shell.execute_reply":"2025-09-24T15:18:25.370390Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\n\ndata_collator = DataCollatorForOCR(processor=processor)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=processor.tokenizer,  # still needed for generation & decoding\n    data_collator=data_collator    # custom collator here\n)\n\n\n# Start training\ntrainer.train()\n\n# Save model & processor after training\nmodel.save_pretrained(\"work/trocr_finetuned_model\")\nprocessor.save_pretrained(\"work/trocr_finetuned_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:18:25.371673Z","iopub.execute_input":"2025-09-24T15:18:25.371937Z","iopub.status.idle":"2025-09-24T15:49:23.055588Z","shell.execute_reply.started":"2025-09-24T15:18:25.371915Z","shell.execute_reply":"2025-09-24T15:49:23.054870Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/172896966.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13620/13620 30:48, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1362</td>\n      <td>2.387700</td>\n    </tr>\n    <tr>\n      <td>2724</td>\n      <td>1.936900</td>\n    </tr>\n    <tr>\n      <td>4086</td>\n      <td>1.760200</td>\n    </tr>\n    <tr>\n      <td>5448</td>\n      <td>1.664000</td>\n    </tr>\n    <tr>\n      <td>6810</td>\n      <td>1.569100</td>\n    </tr>\n    <tr>\n      <td>8172</td>\n      <td>1.465900</td>\n    </tr>\n    <tr>\n      <td>9534</td>\n      <td>1.371900</td>\n    </tr>\n    <tr>\n      <td>10896</td>\n      <td>1.302300</td>\n    </tr>\n    <tr>\n      <td>12258</td>\n      <td>1.215300</td>\n    </tr>\n    <tr>\n      <td>13620</td>\n      <td>1.154200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import VisionEncoderDecoderModel, TrOCRProcessor\nimport editdistance\n\n# === Load Fine-Tuned Model and Processor ===\nmodel_path = \"work/trocr_finetuned_model\"\n\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path)\nprocessor = TrOCRProcessor.from_pretrained(model_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n\n# === CER Metric Function ===\ndef compute_cer(preds, targets):\n    \"\"\"\n    Computes Character Error Rate (CER) between predicted and target texts.\n    \"\"\"\n    assert len(preds) == len(targets), \"Predictions and targets must be the same length.\"\n\n    total_edits = 0\n    total_chars = 0\n\n    for pred, target in zip(preds, targets):\n        pred = pred.strip().replace(\" \", \"\")\n        target = target.strip().replace(\" \", \"\")\n        total_edits += editdistance.eval(pred, target)\n        total_chars += len(target)\n\n    if total_chars == 0:\n        return 0.0\n\n    return total_edits / total_chars\n\n# === CER Scoring Function ===\ndef evaluate_model_cer(preds, targets):\n    \"\"\"\n    Computes CER and assigns a score based on the project rubric.\n    \"\"\"\n    cer_score = compute_cer(preds, targets)\n    cer_percent = cer_score * 100\n\n    if cer_percent <= 43:\n        score = 50\n    elif cer_percent <= 45:\n        score = 40\n    elif cer_percent <= 47:\n        score = 30\n    else:\n        score = 0\n\n    print(f\"🔍 CER: {cer_percent:.2f}%\")\n    print(f\"📊 Score Based on CER: {score} / 50\")\n\n    return cer_score, score\n\n# === Inference and Evaluation ===\n# Assuming val_dataset is already defined (see training section)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\npredictions = []\nground_truths = []\n\nfor batch in val_dataloader:\n    pixel_values = batch[\"pixel_values\"].to(device)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n            pixel_values,\n            max_length=64,\n            num_beams=4,\n            early_stopping=True,\n            no_repeat_ngram_size=3,\n        )\n\n    # Decode predictions and ground truths\n    pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    true_text = processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)[0]\n\n    predictions.append(pred_text)\n    ground_truths.append(true_text)\n\n# === Compute CER and Score ===\ncer, cer_score = evaluate_model_cer(predictions, ground_truths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:49:23.056509Z","iopub.execute_input":"2025-09-24T15:49:23.057230Z","iopub.status.idle":"2025-09-24T15:50:07.793240Z","shell.execute_reply.started":"2025-09-24T15:49:23.057204Z","shell.execute_reply":"2025-09-24T15:50:07.792396Z"}},"outputs":[{"name":"stdout","text":"🔍 CER: 70.39%\n📊 Score Based on CER: 0 / 50\n","output_type":"stream"}],"execution_count":13}]}